{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cuda.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew0db6MBtK6N",
        "outputId": "42fe9589-66da-4950-c638-ceb2449c2534"
      },
      "source": [
        "!apt update -qq;\n",
        "!wget https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb;\n",
        "!dpkg -i cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb;\n",
        "!apt-key add /var/cuda-repo-8-0-local-ga2/7fa2af80.pub;\n",
        "!apt-get update -qq;\n",
        "!apt-get install cuda gcc-5 g++-5 -y -qq;\n",
        "!ln -s /usr/bin/gcc-5 /usr/local/cuda/bin/gcc;\n",
        "!ln -s /usr/bin/g++-5 /usr/local/cuda/bin/g++;\n",
        "!apt install cuda-8.0;"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "--2021-08-06 13:20:27--  https://developer.nvidia.com/compute/cuda/8.0/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.0.24\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.0.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://developer.nvidia.com/compute/cuda/8.0/prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb [following]\n",
            "--2021-08-06 13:20:27--  https://developer.nvidia.com/compute/cuda/8.0/prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb\n",
            "Reusing existing connection to developer.nvidia.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?msuRZ_ph7mGMplz5saKcxjBVuj05wCO_OLFBZSuuFjcYpZ1E0P16YIaguq8R21v5vJzfyqGG_j17Bp2nIsr_sOpxrMiV_Uzu1Fq2NYVNcQvdpHJoigQlgVg-_bV15teuucS78zYgz2fputHO0AXUlefM8ny1GqUKd7ddakRaZuZM40k5TuuNKS0JIh_1knJ-_qDz17UwRBrCzNGS_9Al9TaB3g [following]\n",
            "--2021-08-06 13:20:28--  https://developer.download.nvidia.com/compute/cuda/8.0/secure/Prod2/local_installers/cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64.deb?msuRZ_ph7mGMplz5saKcxjBVuj05wCO_OLFBZSuuFjcYpZ1E0P16YIaguq8R21v5vJzfyqGG_j17Bp2nIsr_sOpxrMiV_Uzu1Fq2NYVNcQvdpHJoigQlgVg-_bV15teuucS78zYgz2fputHO0AXUlefM8ny1GqUKd7ddakRaZuZM40k5TuuNKS0JIh_1knJ-_qDz17UwRBrCzNGS_9Al9TaB3g\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1913589814 (1.8G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb.1’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.78G  35.7MB/s    in 14s     \n",
            "\n",
            "2021-08-06 13:20:42 (128 MB/s) - ‘cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb.1’ saved [1913589814/1913589814]\n",
            "\n",
            "(Reading database ... 177966 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-8-0-local-ga2_8.0.61-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) over (8.0.61-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-8-0-local-ga2 (8.0.61-1) ...\n",
            "Warning: The postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2\n",
            "Warning: seems to use apt-key (provided by apt) without depending on gnupg or gnupg2.\n",
            "Warning: This will BREAK in the future and should be fixed by the package maintainer(s).\n",
            "Note: Check first if apt-key functionality is needed at all - it probably isn't!\n",
            "Warning: apt-key should not be used in scripts (called from postinst maintainerscript of the package cuda-repo-ubuntu1604-8-0-local-ga2)\n",
            "OK\n",
            "OK\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Note, selecting 'cuda-8-0' for regex 'cuda-8.0'\n",
            "Note, selecting 'libcuda-8.0-1' for regex 'cuda-8.0'\n",
            "cuda-8-0 is already the newest version (8.0.61-1).\n",
            "The following package was automatically installed and is no longer required:\n",
            "  nsight-compute-2021.2.0\n",
            "Use 'apt autoremove' to remove it.\n",
            "0 upgraded, 0 newly installed, 0 to remove and 74 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raZP-DVg1dLr",
        "outputId": "41db7cda-acb5-466c-e28c-d0c5fc684c75"
      },
      "source": [
        "!pip install git+git://github.com/andreinechaev/nvcc4jupyter.git"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/andreinechaev/nvcc4jupyter.git\n",
            "  Cloning git://github.com/andreinechaev/nvcc4jupyter.git to /tmp/pip-req-build-7opygm75\n",
            "  Running command git clone -q git://github.com/andreinechaev/nvcc4jupyter.git /tmp/pip-req-build-7opygm75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hIYCR0bO18yn",
        "outputId": "616e3090-e235-4c50-8f08-cbfa0db61c30"
      },
      "source": [
        "%load_ext nvcc_plugin"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The nvcc_plugin extension is already loaded. To reload it, use:\n",
            "  %reload_ext nvcc_plugin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3mHjGNPzRyf"
      },
      "source": [
        "# Test CUDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYNle52KtH8-",
        "outputId": "8ab7a5f5-03ef-4d14-8592-8f595d48a120"
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "int main() {\n",
        "    std::cout << \"Hello world\\n\";\n",
        "    return 0;\n",
        "}"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello world\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJKVf4AW32Q3"
      },
      "source": [
        "# Array Sum Problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ghq6v-6UtGzb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "057e5df1-1118-416e-fe8c-8f71f1797bc5"
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "__global__ void add(int n, float *x, float *y)\n",
        "{\n",
        "  for (int i = 0; i < n; i++)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        " \n",
        "  int N = 1<<20;\n",
        "\n",
        " float *x, *y;\n",
        "\n",
        "\n",
        " cudaMallocManaged(&x, N*sizeof(float));\n",
        " cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        " \n",
        "  add<<<1,1>>>(N, x, y); \n",
        "\n",
        " cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        " \n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        " \n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max error: 0\n",
            "src/tcmalloc.cc:283] Attempt to free invalid pointer 0x7fa304000000 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaNOtGyG51JP"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 1.cu -o 1 -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpKOx-5H541W",
        "outputId": "7e47542c-01ca-4bad-a21f-753b0ac86438"
      },
      "source": [
        "!nvprof ./1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Warning: This version of nvprof doesn't support the underlying device, GPU profiling skipped.\n",
            "Max error: 0\n",
            "src/tcmalloc.cc:283] Attempt to free invalid pointer 0x7ff63a000000 \n",
            "======== Warning: No CUDA application was profiled, exiting\n",
            "======== Error: Application received signal 134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa3UCicr9t_o"
      },
      "source": [
        "To make out program realy fast, like not just using the GPU, we want it to realy use all the architecture of a GPU and in order to do that we need to use some of those GPU constructs that `Nvidia` has defined like threads, blocks, and grids.\n",
        "\n",
        "Let's check that:\n",
        "\n",
        "Neural Netoworks use matrix operations all the time and we nknow that properly.\n",
        "\n",
        "Now to increase our speed we can use GPU. Although CPU supports multithreading, but it is not supporting a million thread. That's what GPU are made for.\n",
        "\n",
        "Let's look at GPU. On the GPU, each single thread lives inside of what's called a block. Each block has multiple of 32 threads in itself. Also a set of blocks makes grid. That means you have `a grid of blocks of threads`.\n",
        "\n",
        "\n",
        "The reason for limit to the number of threads per grid is because of memories. Global memory and shared memories in each block.\n",
        "\n",
        "What we want to do is we want to say we need 1M threads for our array sum. Because there is a memory limit for each of these blocks then we know that we want the maximum amount of threads per block and we want the max amount of blocks that come out to 1M threads. How we do that? We can define this as a simple function:\n",
        "\n",
        "```\n",
        "numBlock = (N + blockSize - 1)/blockSize\n",
        "```\n",
        "\n",
        "Then we can run our function in the way that we want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oY0keuI0EuFF",
        "outputId": "87a9695e-0f47-4580-9948-1a0de4e401df"
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "__global__ void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        " \n",
        "  int N = 1<<20;\n",
        "\n",
        " float *x, *y;\n",
        "\n",
        "\n",
        " cudaMallocManaged(&x, N*sizeof(float));\n",
        " cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        " \n",
        "  add<<<1,256>>>(N, x, y); \n",
        "\n",
        " cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        " \n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        " \n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max error: 0\n",
            "src/tcmalloc.cc:283] Attempt to free invalid pointer 0x7fcad4000000 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9zoEvqUKFqRE"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true 2.cu -o 2 -lcudadevrt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUrICgxUFtej",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0be2d94-0b93-4984-d8ab-d939239a1e96"
      },
      "source": [
        "!nvprof ./2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "======== Warning: This version of nvprof doesn't support the underlying device, GPU profiling skipped.\n",
            "Max error: 0\n",
            "src/tcmalloc.cc:283] Attempt to free invalid pointer 0x7fdbb6000000 \n",
            "======== Warning: No CUDA application was profiled, exiting\n",
            "======== Error: Application received signal 134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dETCN2wFuhg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5a074be-7ae5-4fc0-f5c8-dcbe839a56d2"
      },
      "source": [
        "%%cu\n",
        "#include <iostream>\n",
        "#include <math.h>\n",
        "\n",
        "// function to add the elements of two arrays\n",
        "__global__ void add(int n, float *x, float *y)\n",
        "{\n",
        "  int index = threadIdx.x;\n",
        "  int stride = blockDim.x;\n",
        "\n",
        "  for (int i = index; i < n; i += stride)\n",
        "      y[i] = x[i] + y[i];\n",
        "}\n",
        "\n",
        "int main(void)\n",
        "{\n",
        " \n",
        "  int N = 1<<20;\n",
        "\n",
        " float *x, *y;\n",
        "\n",
        "\n",
        " cudaMallocManaged(&x, N*sizeof(float));\n",
        " cudaMallocManaged(&y, N*sizeof(float));\n",
        "\n",
        "\n",
        "  // initialize x and y arrays on the host\n",
        "  for (int i = 0; i < N; i++) {\n",
        "    x[i] = 1.0f;\n",
        "    y[i] = 2.0f;\n",
        "  }\n",
        " \n",
        "  int blockSize = 256;\n",
        "  int numBlocks = (N + blockSize - 1) / blockSize;\n",
        "\n",
        "  add<<<numBlocks,blockSize>>>(N, x, y); \n",
        "\n",
        " cudaDeviceSynchronize();\n",
        "\n",
        "  // Check for errors (all values should be 3.0f)\n",
        "  float maxError = 0.0f;\n",
        "  for (int i = 0; i < N; i++)\n",
        "    maxError = fmax(maxError, fabs(y[i]-3.0f));\n",
        "  std::cout << \"Max error: \" << maxError << std::endl;\n",
        " \n",
        "\n",
        "  // Free memory\n",
        "  delete [] x;\n",
        "  delete [] y;\n",
        " \n",
        "\n",
        "  return 0;\n",
        "}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max error: 0\n",
            "src/tcmalloc.cc:283] Attempt to free invalid pointer 0x7faff8000000 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oh2jtauoPdz"
      },
      "source": [
        "# Neural Network in CUDA\n",
        "\n",
        "Now Let;s train a simple neural network on `Iris-Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DN02iwCFoO1Y"
      },
      "source": [
        "%%cu\n",
        "\n",
        "#include <stdio.h>\n",
        "\n",
        "__global__ void kMartixByMatrixElementwise(const int nThreads, const float *m1, const float *m2, float *output) {\n",
        "    /*  Computes the product of two arrays (elementwise multiplication).\n",
        "     Inputs:\n",
        "     m1: array\n",
        "     m2: array\n",
        "     output: array,the results of the multiplication are to be stored here\n",
        "    */\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = m1[i] * m2[i];\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dMartixByMatrixElementwise(const float *m1, const float *m2, float *output, const int width, const int height){\n",
        "\n",
        "\tkMartixByMatrixElementwise <<< width, height >>> ( width * height, m1, m2, output );\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}\n",
        "\n",
        "__global__ void kMartixSubstractMatrix(const int nThreads, const float *m1, const float *m2, float *output) {\n",
        "    /*  Computes the (elementwise) difference between two arrays\n",
        "     Inputs:\n",
        "     m1: array\n",
        "     m2: array\n",
        "     output: array,the results of the computation are to be stored here\n",
        "     */\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = m1[i] - m2[i];\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dMartixSubstractMatrix(const float *m1, const float *m2, float *output, const int width, const int height){\n",
        "\n",
        "\tkMartixSubstractMatrix <<< width, height >>> ( width * height, m1, m2, output );\n",
        "    cudaDeviceSynchronize();\n",
        "    return output;\n",
        "}\n",
        "\n",
        "__global__ void kSigmoid(const int nThreads, float const *input, float *output){\n",
        "    /*  Computes the value of the sigmoid function f(x) = 1/(1 + e^-x).\n",
        "     Inputs:\n",
        "     input: array\n",
        "     output: array, the results of the computation are to be stored here\n",
        "    */\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = 1.0 / (1.0 + std::exp(-input[i]));\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ void dSigmoid(float const *input, float *output, const int height, const int width){\n",
        "\n",
        "\tkSigmoid <<< height, width >>> (height * width, input, output);\n",
        "\tcudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "__global__ void kSigmoid_d(const int nThreads, float const *input, float *output) {\n",
        "\t/*  Computes the value of the sigmoid function derivative f'(x) = f(x)(1 - f(x)),\n",
        "\t    where f(x) is sigmoid function.\n",
        "\t    Inputs:\n",
        "\t    input: array\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tx(1 - x) for every element of the input matrix m1.\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t  {\n",
        "\t\toutput[i] = input[i] * (1 - input[i]);\n",
        "\t  }\n",
        "}\n",
        "\n",
        "__device__ float* dSigmoid_d(float const *input, float *output, const int rows, const int columns){\n",
        "\tkSigmoid_d <<< rows, columns >>> (rows*columns, input, output);\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot(const int nThreads, const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_columns ){\n",
        "\t/*  Computes the product of two matrices: m1 x m2.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns\n",
        "\t    m2: array, right matrix of size m1_columns x m2_columns (the number of rows in the right matrix\n",
        "\t    must be equal to the number of the columns in the left one)\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_rows x m2_columns\n",
        "\t    m1_rows: int, number of rows in the left matrix m1\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_columns: int, number of columns in the right matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t    int r = (int)i / m2_columns;\n",
        "\t    int c = i % m2_columns;\n",
        "\t    float t_output = 0.f;\n",
        "\n",
        "\t    for( int k = 0; k < m1_columns; ++k ) {\n",
        "\t        t_output += m1[ r * m1_columns + k ] * m2[ k * m2_columns + c ];\n",
        "\t    }\n",
        "\n",
        "\t    output[i] = t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ float* dDot(const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_columns ){\n",
        "\n",
        "\tkDot <<< m1_rows, m2_columns >>> (m1_rows * m2_columns, m1, m2, output, m1_rows , m1_columns, m2_columns );\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot_m1_m2T(const int nThreads, const float *m1, const float *m2, float *output, const int m1_columns, const int m2_rows ){\n",
        "\t/*  Updates the output matrix with the product of two matrices: m1 and m2 transposed.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns\n",
        "\t    m2: array, right matrix of size m2_rows x m1_columns (m2 transposed will be of size m1_columns x m2_rows)\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_rows x m2_rows\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_rows: int, number of rows in the left matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t\tint r = (int)i / m2_rows;\n",
        "\t\tint c = i % m2_rows;\n",
        "\t\tfloat t_output = 0.0;\n",
        "\t\tint id_T;\n",
        "\n",
        "\t\tfor( int k = 0; k < m1_columns; ++k ) {\n",
        "\t\t\tid_T = c * m1_columns + k;\n",
        "\t\t\tt_output += m1[ r * m1_columns + k ] * m2[ id_T ];\n",
        "\t\t}\n",
        "\n",
        "\t\toutput[i] = t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ float* dDot_m1_m2T(const float *m1, const float *m2, float *output, const int m1_rows , const int m1_columns, const int m2_rows )\n",
        "{\n",
        "\tkDot_m1_m2T <<< m1_rows, m2_rows >>> ( m1_rows * m2_rows, m1, m2, output, m1_columns, m2_rows );\n",
        "\tcudaDeviceSynchronize();\n",
        "\treturn output;\n",
        "}\n",
        "\n",
        "__global__ void kDot_m1T_m2(const int nThreads, const float *m1, const float *m2, float *output, const int m1_rows,\n",
        "\t\t\t\t\t\t\tconst int m1_columns, const int m2_columns ){\n",
        "\t/*  Increments the output matrix with the product of two matrices: m1 transposed and m2.\n",
        "\t   \tInputs:\n",
        "\t    m1: array, left matrix of size m1_rows x m1_columns (m1 transposed will be of size m1_columns x m1_rows)\n",
        "\t    m2: array, right matrix of size m1_rows x m2_columns\n",
        "\t    output: array, the results of the computation are to be stored here:\n",
        "\t    \t\tm1 * m2, product of two arrays m1 and m2, a matrix of size m1_columns x m2_columns\n",
        "\t    m1_rows: int, number of rows in the left matrix m1\n",
        "\t    m1_columns: int, number of columns in the left matrix m1\n",
        "\t    m2_rows: int, number of rows in the left matrix m2\n",
        "\t*/\n",
        "\n",
        "\tfor (int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "\t\t i < nThreads;\n",
        "\t\t i += blockDim.x * gridDim.x)\n",
        "\t{\n",
        "\t    int r = (int)i / m2_columns;\n",
        "\t    int c = i % m2_columns;\n",
        "\t    int id_T;\n",
        "\t    float t_output = 0.0;\n",
        "\n",
        "\t    for( int k = 0; k < m1_rows; ++k ) {\n",
        "\t    \tid_T = k * m1_columns + r;\n",
        "\t        t_output += m1[ id_T ] * m2[ k * m2_columns + c ];\n",
        "\t    }\n",
        "\n",
        "\t    output[i] += t_output;\n",
        "\t}\n",
        "}\n",
        "\n",
        "__device__ void dDot_m1T_m2(const float *m1, const float *m2, float *output, const int m1_height , const int m1_width, const int m2_width )\n",
        "{\n",
        "\tkDot_m1T_m2 <<< m1_width, m2_width >>> (m1_width * m2_width, m1, m2, output, m1_height, m1_width, m2_width );\n",
        "\tcudaDeviceSynchronize();\n",
        "}\n",
        "\n",
        "__device__ void kPrintMatrix (const float* M, int h, int w) {\n",
        "    /*  Prints out the input array as h x w matrix.\n",
        "     Inputs:\n",
        "     m: vector, matrix of size n_rows x n_columns\n",
        "     h: int, number of rows in the matrix M\n",
        "     w: int, number of columns in the matrix M\n",
        "     */\n",
        "\tfor (int i = 0; i < h; i++){\n",
        "\t\tfor (int j = 0; j < w; j++){\n",
        "\t\t\tprintf(\"%f  \", M[i*w+j]);\n",
        "\t\t}\n",
        "\t\tprintf(\"\\n\");\n",
        "\t}\n",
        "\tprintf(\"\\n\");\n",
        "}\n",
        "\n",
        "__global__ void kFit(\tconst float* X, const int X_w, const int X_h,\n",
        "\t\t\t\t\t\tconst float* y, const int y_w,\n",
        "\t\t\t\t\t\tfloat* l1, const int l1_w, float* l_1_d,\n",
        "\t\t\t\t\t\tfloat* pred, float* pred_d,\n",
        "\t\t\t\t\t\tfloat* W0,\n",
        "\t\t\t\t\t\tfloat* W1,\n",
        "\t\t\t\t\t\tfloat* buffer\n",
        "\t\t\t\t\t\t)\n",
        "{\n",
        "\tfor (unsigned i = 0; i < 50; ++i) {\n",
        "\n",
        "        dSigmoid(dDot(X, W0, l1, X_h, X_w, l1_w), l1, X_h, l1_w);\n",
        "        dSigmoid(dDot(l1, W1, pred, X_h, l1_w, y_w), pred, X_h, y_w);\n",
        "        dMartixByMatrixElementwise(dMartixSubstractMatrix(y, pred, pred_d, X_h, y_w), dSigmoid_d(pred, buffer, X_h, y_w), pred_d, X_h, y_w );\n",
        "        dMartixByMatrixElementwise(dDot_m1_m2T(pred_d, W1, l_1_d, X_h, y_w, l1_w), dSigmoid_d(l1, buffer, X_h, l1_w), l_1_d, X_h, l1_w);\n",
        "        dDot_m1T_m2( l1, pred_d, W1, X_h, l1_w, y_w );\n",
        "        dDot_m1T_m2( X, l_1_d, W0, X_h, X_w, l1_w );\n",
        "    }\n",
        "}\n",
        "\n",
        "int main(void){\n",
        "\n",
        "\tconst int TRAINING_SIZE = 4;\n",
        "\tconst int TRAINING_DIM = 4;\n",
        "\tconst int L1_SIZE = 8;\n",
        "\n",
        "\t// X, the first 4 lines from Iris dataset\n",
        "\tfloat h_X[TRAINING_SIZE*TRAINING_DIM] = {\t5.1, 3.5, 1.4, 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t4.9, 3.0, 1.4, 0.2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t6.2, 3.4, 5.4, 2.3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t5.9, 3.0, 5.1, 1.8 };\n",
        "\n",
        "\tconst signed int X_size = sizeof(h_X);\n",
        "\n",
        "\tfloat *d_X;\n",
        "\tcudaMalloc(&d_X, X_size);\n",
        "\tcudaMemcpy(d_X, h_X, X_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//WEIGHTS_0\n",
        "\tconst long signed int W0_size = L1_SIZE*TRAINING_DIM*sizeof(float);\n",
        "\tfloat *h_W0 = (float*)malloc(W0_size);\n",
        "\tfor (int i = 0; i < L1_SIZE*TRAINING_DIM; i++){\n",
        "\t    h_W0[i] = 0.1 * (2.0*rand()/RAND_MAX-1.0);\n",
        "\t}\n",
        "\n",
        "\tfloat *d_W0;\n",
        "\tcudaMalloc(&d_W0, W0_size);\n",
        "\tcudaMemcpy(d_W0, h_W0, W0_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//LAYER_1, LAYER_1_DELTA AND BUFFER OF LAYER 1 SIZE\n",
        "\tconst long signed int L1_size = L1_SIZE*TRAINING_SIZE*sizeof(float);\n",
        "\n",
        "\tfloat* h_layer_1 = (float*)malloc(L1_size);\n",
        "\tfloat* h_layer_1_delta = (float*)malloc(L1_size);\n",
        "\tfloat* h_buffer = (float*)malloc(L1_size);\n",
        "\n",
        "\tfor (int i = 0; i < L1_SIZE*TRAINING_SIZE; i++){\n",
        "\t    h_layer_1[i] = 0.0;\n",
        "\t    h_buffer[i] = 0.0;\n",
        "\t    h_layer_1_delta[i] = 0.0;\n",
        "\t}\n",
        "\n",
        "\tfloat *d_layer_1;\n",
        "\tcudaMalloc(&d_layer_1, L1_size);\n",
        "\tcudaMemcpy(d_layer_1, h_layer_1, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_buffer;\n",
        "\tcudaMalloc(&d_buffer, L1_size);\n",
        "\tcudaMemcpy(d_buffer, h_buffer, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_layer_1_delta;\n",
        "\tcudaMalloc(&d_layer_1_delta, L1_size);\n",
        "\tcudaMemcpy(d_layer_1_delta, h_layer_1_delta, L1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//WEIGHTS_1\n",
        "\tconst long signed int W1_size = L1_SIZE*sizeof(float);\n",
        "\tfloat *h_W1 = (float*)malloc(W1_size);\n",
        "\tfor (int i = 0; i < L1_SIZE; i++){\n",
        "\t    h_W1[i] = 0.1* (2.0*rand()/RAND_MAX-1.0);\n",
        "\t}\n",
        "\n",
        "\tfloat *d_W1;\n",
        "\tcudaMalloc(&d_W1, W1_size);\n",
        "\tcudaMemcpy(d_W1, h_W1, W1_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//Y\n",
        "\tfloat h_y[4] = {\t0,\n",
        "\t\t\t\t\t\t0,\n",
        "\t\t\t\t\t\t1,\n",
        "\t\t\t\t\t\t1 };\n",
        "\tconst signed int y_size = sizeof(h_y);\n",
        "\tfloat *d_y;\n",
        "\tcudaMalloc(&d_y, y_size);\n",
        "\tcudaMemcpy(d_y, h_y, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\t//PRED AND PRED_DELTA\n",
        "\tfloat* h_pred = (float*)malloc(y_size);\n",
        "\tfloat* h_pred_delta = (float*)malloc(y_size);\n",
        "\tfor (int i = 0; i < TRAINING_SIZE; i++){\n",
        "\t    h_pred[i] = 0.0;\n",
        "\t    h_pred_delta[i] = 0.0;\n",
        "\t}\n",
        "\n",
        "\tfloat *d_pred;\n",
        "\tcudaMalloc(&d_pred, y_size);\n",
        "\tcudaMemcpy(d_pred, h_pred, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tfloat *d_pred_delta;\n",
        "\tcudaMalloc(&d_pred_delta, y_size);\n",
        "\tcudaMemcpy(d_pred_delta, h_pred_delta, y_size, cudaMemcpyHostToDevice);\n",
        "\n",
        "\tkFit <<< 1, 1 >>> (\td_X, TRAINING_DIM, TRAINING_SIZE,\n",
        "\t\t\t\t\t\td_y, 1,\n",
        "\t\t\t\t\t\td_layer_1, L1_SIZE, d_layer_1_delta,\n",
        "\t\t\t\t\t\td_pred,\n",
        "\t\t\t\t\t\td_pred_delta,\n",
        "\t\t\t\t\t\td_W0,\n",
        "\t\t\t\t\t\td_W1,\n",
        "\t\t\t\t\t\td_buffer);\n",
        "\n",
        "\tcudaMemcpy(h_pred, d_pred, y_size, cudaMemcpyDeviceToHost);\n",
        "\n",
        "\tcudaFree(d_pred);\n",
        "\tcudaFree(d_X);\n",
        "\tcudaFree(d_y);\n",
        "\tcudaFree(d_layer_1_delta);\n",
        "\tcudaFree(d_pred_delta);\n",
        "\tcudaFree(d_W0);\n",
        "\tcudaFree(d_W1);\n",
        "\tcudaFree(d_buffer);\n",
        "\n",
        "\tfree(h_layer_1_delta);\n",
        "\tfree(h_pred_delta);\n",
        "\tfree(h_W0);\n",
        "\tfree(h_W1);\n",
        "\tfree(h_buffer);\n",
        "\n",
        "\tfor (int i = 0; i < TRAINING_SIZE; i++){\n",
        "\t\tprintf(\"Prediction[%i] : %f True Value[%i] : %f Error[%i] : %f\\n\", i, h_pred[i], i, h_y[i], i, h_pred[i] - h_y[i]);\n",
        "\t}\n",
        "\n",
        "\tfree(h_pred);\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em6budhrqJrc"
      },
      "source": [
        "!/usr/local/cuda/bin/nvcc -arch=sm_35 -rdc=true IrisNN.cu -o IrisNN -lcudadevrt"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "82inzsJmqW7i",
        "outputId": "3d158ca1-8b00-4501-b054-8fec2ee8d0ed"
      },
      "source": [
        "!./IrisNN"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction[0] : 0.060997 True Value[0] : 0.000000 Error[0] : 0.060997\n",
            "Prediction[1] : 0.076193 True Value[1] : 0.000000 Error[1] : 0.076193\n",
            "Prediction[2] : 0.927551 True Value[2] : 1.000000 Error[2] : -0.072449\n",
            "Prediction[3] : 0.918263 True Value[3] : 1.000000 Error[3] : -0.081737\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}